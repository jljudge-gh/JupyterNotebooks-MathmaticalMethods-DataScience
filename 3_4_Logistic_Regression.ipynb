{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.4 Logistic Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOHnfQAxso4OTug7dxG1gaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jljudge-gh/JupyterNotebooks-MathmaticalMethods-DataScience/blob/main/3_4_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHU4rhLQptZy"
      },
      "source": [
        "# 3.4 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHCKCJ6Aq8aR"
      },
      "source": [
        "Logistic regression is a model that in its basic form uses a logistic function\n",
        "to model a binary dependent variable. It can be extended to several classes\n",
        "of events such as classfication of images. In this section, we illustrate the use of gradient descent on binary classification by logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFC4trlq-Vm"
      },
      "source": [
        "Given the input data is of the form $\\left\\{\\left(\\alpha_{i}, b_{i}\\right): i=1, \\ldots, n\\right\\}$ where $\\alpha_{i} \\in \\mathbb{R}^{d}$ are the features and $b_{i} \\in\\{0,1\\}$ is the label. As before we use a matrix representation: $A \\in \\mathbb{R}^{n \\times d}$ has rows $\\alpha_{j}^{T}, j=1, \\ldots, n$ and $\\mathbf{b}=\\left(b_{1}, \\ldots, b_{n}\\right)^{T} \\in\\{0,1\\}^{n}$. We wish to find a function of the features that approximates the probability of the label 1. For this purpose, we model the logit function of the probability of label 1 as a linear function of the features. Figure $3.11$ is the graph of the logit function.\n",
        "\n",
        "For $\\mathbf{x}, \\alpha \\in \\mathbb{R}^{d}$, let $p(\\alpha ; \\mathbf{x})$ be the probability of the output to be 1 , we define"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cbID6C_rlP7"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU994Hl9rZxV"
      },
      "source": [
        "$$\n",
        "\\log \\frac{p(\\alpha ; \\mathbf{x})}{1-p(\\alpha ; \\mathbf{x})}=\\alpha^{T} \\mathbf{x}\n",
        "$$\n",
        "Here $\\alpha^{T} \\mathbf{x}=\\sum x_{i} \\alpha_{i}$ can be viewed as a regression problem which seeks the best parameters ( $\\mathbf{x}$ ) with given data $(\\alpha)$. Rearranging this expression gives\n",
        "$$\n",
        "p(\\alpha ; \\mathbf{x})=\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\n",
        "$$\n",
        "where the sigmoid function is\n",
        "$$\n",
        "\\sigma(t)=\\frac{1}{1+e^{-t}}\n",
        "$$\n",
        "for $t \\in \\mathbb{R}$. To maximize the likelihood of the data, we assume the labels are independent given the features, which is given by\n",
        "$$\n",
        "\\mathscr{L}(\\mathbf{x} ; A, \\mathbf{b})=\\prod_{i=1}^{n} p\\left(\\alpha_{i} ; \\mathbf{x}\\right)^{b_{i}}\\left(1-p\\left(\\alpha_{i} ; \\mathbf{x}\\right)\\right)^{1-b_{i}}\n",
        "$$\n",
        "Taking a logarithm, multiplying by $-1 / n$ and substituting the sigmoid function, we want to minimize the cross-entropy loss.\n",
        "$$\n",
        "\\ell(\\mathbf{x} ; A, \\mathbf{b})=-\\frac{1}{n} \\sum_{i=1}^{n} b_{i} \\log \\left(\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\\right)-\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-b_{i}\\right) \\log \\left(1-\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\\right)\n",
        "$$\n",
        "That is, we solve\n",
        "$$\n",
        "\\min _{\\mathbf{x} \\in \\mathbb{R}^{d}} \\ell(\\mathbf{x} ; A, \\mathbf{b})\n",
        "$$\n",
        "To use gradient descent, we need to compute the gradient of $\\ell$. We use the Chain Rule and first compute the derivative of $\\sigma$ which is\n",
        "$$\n",
        "\\sigma^{\\prime}(t)=\\frac{e^{-t}}{\\left(1+e^{-t}\\right)^{2}}=\\frac{1}{1+e^{-t}}\\left(1-\\frac{1}{1+e^{-t}}\\right)=\\sigma(t)(1-\\sigma(t))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh51E8k5rwAb"
      },
      "source": [
        "It follows that $\\sigma(t)$ satisfies the logistic differential equation. It arises in a variety of applications, including the modeling of population dynamics. Here it will be a convenient way to compute the gradient. Indeed observe that by the Chain Rule\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}} \\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)=\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\\left(1-\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\\right) \\alpha\n",
        "$$\n",
        "where we use a subscript $\\mathbf{x}$ to make it clear that the gradient is with respect to $\\mathbf{x}$.\n",
        "With the same approach, we have\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla_{\\mathbf{x}} \\ell(\\mathbf{x} ; A, \\mathbf{b}) &=-\\frac{1}{n} \\sum_{i=1}^{n} \\frac{b_{i}}{\\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)} \\nabla_{\\mathbf{x}} \\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)+\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1-b_{i}}{1-\\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)} \\nabla_{\\mathbf{x}} \\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right) \\\\\n",
        "&=-\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\frac{b_{i}}{\\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)}-\\frac{1-b_{i}}{1-\\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)}\\right) \\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)\\left(1-\\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)\\right) \\alpha_{i} \\\\\n",
        "&=-\\frac{1}{n} \\sum_{i=1}^{n}\\left(b_{i}-\\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)\\right) \\alpha_{i}\n",
        "\\end{aligned}\n",
        "$$\n",
        "To compute the Hessian, we note that\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}}\\left(\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right) \\alpha_{j}\\right)=\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\\left(1-\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\\right) \\alpha \\alpha_{j}\n",
        "$$\n",
        "so that\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}}\\left(\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right) \\alpha\\right)=\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\\left(1-\\sigma\\left(\\alpha^{T} \\mathbf{x}\\right)\\right) \\alpha \\alpha^{T}\n",
        "$$\n",
        "Thus\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}}^{2} \\ell(\\mathbf{x} ; A, \\mathbf{b})=\\frac{1}{n} \\sum_{i=1}^{n} \\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)\\left(1-\\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}\\right)\\right) \\alpha_{i} \\alpha_{i}^{T}\n",
        "$$\n",
        "where $\\nabla_{\\mathbf{x}}^{2}$ indicates the Hessian with respect to the $\\mathbf{x}$ variables. Now each $\\alpha_{i} \\alpha_{i}^{T}$ is a symmetric matrix and PSD. As a result, the function $\\ell(\\mathbf{x} ; A, \\mathbf{b})$ is convex as a function of $\\mathbf{x} \\in \\mathbb{R}^{d}$. We want to comment that convexity is one reason for working with the cross-entropy loss rather than the mean squared error.\n",
        "\n",
        "To update iteration formula: for step size $\\beta$, one step of gradient descent is therefore\n",
        "$$\n",
        "\\mathbf{x}^{k+1}=\\mathbf{x}^{k}+\\beta \\frac{1}{n} \\sum_{i=1}^{n}\\left(b_{i}-\\sigma\\left(\\alpha_{i}^{T} \\mathbf{x}^{k}\\right)\\right) \\alpha_{i}\n",
        "$$\n",
        "In stochastic gradient descent, a variant of gradient descent, we pick a sample $I$ uniformly at random in $\\{1, \\ldots, n\\}$ and update as follows\n",
        "$$\n",
        "\\mathbf{x}^{k+1}=\\mathbf{x}^{k}+\\beta\\left(b_{I}-\\sigma\\left(\\alpha_{I}^{T} \\mathbf{x}^{k}\\right)\\right) \\alpha_{I}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uasyYRsOue7X"
      },
      "source": [
        "## Logistic Regression Formula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbvhy4KRuqGy"
      },
      "source": [
        "Logistic Regression in its base form (by default) is a Binary Classifier. This means that the target vector may only take the form of one of two values. In the Logistic Regression Algorithm formula, we have a Linear Model, e.g., $\\beta_{0}+\\beta_{1} x$, that is integrated into a Logistic Function (also known as a Sigmoid Function). The Binary Classifier formula that we have at the end is as follows:\n",
        "$$\n",
        "P\\left(y_{i}=1 \\mid X\\right)=\\frac{1}{1+e^{-\\left(\\beta_{0}+\\beta_{2} x\\right)}}\n",
        "$$\n",
        "Where:\n",
        "- $\\mathrm{P}\\left(\\mathrm{y}_{\\mathrm{i}}=1 \\mid \\mathrm{X}\\right)$ is the probability of the $\\mathrm{i}^{\\text {th }}$ observations target value, $\\mathrm{y}_{\\mathrm{i}}$ belonging to class 1 .\n",
        "- $\\mathrm{B}_{0}$ and $\\beta_{1}$ are the parameters that are to be learned.\n",
        "- erepresents Euler's Number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-1C6Im4ukmF"
      },
      "source": [
        "The Logistic Regression formula aims to limit or constrain the Linear and/or Sigmoid output between a value of 0 and 1. The main reason is for interpretability purposes, i.e., we can read the value as a simple Probability; Meaning that if the value is greater than 0.5 class one would be predicted, otherwise, class 0 is predicted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZzTBrQZu11b"
      },
      "source": [
        "![](https://lh6.googleusercontent.com/N2TdJpFw4S_Sj67bbxdNLdtx9o5G2iVz5Okz6XcTieJNF7WWM0jAXdFIi8g4CtzpVWk3y1SlxdfP-RYzF92Hc1vXVEbRrHIrIG5wi6dPbeYQvk1zNmHcpnUpNzPBxWTInSj4Ywvn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV3Bt_-xrwif"
      },
      "source": [
        "## Logistic Regression in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH4ZCRMatz-S"
      },
      "source": [
        "The following is an implementation of Logistic Regression in Python. For this exercise, we will be using the Ionosphere dataset which is available for download from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/ionosphere)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5vzWBqLvRyB"
      },
      "source": [
        "We begin by importing the necessary packages to be used for the Machine Learning problem\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooiwDVzGvYm-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfIb7mWbvWxk"
      },
      "source": [
        "We read the data into our system using Pandas'\n",
        "'read_csv' method. This transforms the .csv file\n",
        "into a Pandas DataFrame object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qnIGKfKt0Mr"
      },
      "source": [
        "dataframe = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data', header=None)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCWqU5sYvaBj"
      },
      "source": [
        "\n",
        "We configure the display settings of the\n",
        "Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F0kthOovaVk"
      },
      "source": [
        "pd.set_option('display.max_rows', 10000000000)\n",
        "pd.set_option('display.max_columns', 10000000000)\n",
        "pd.set_option('display.width', 95)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVMXI8brvaeQ"
      },
      "source": [
        "We view the shape of the dataframe. Specifically\n",
        "the number of rows and columns present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axv-Ng5kvanx",
        "outputId": "88e92198-7d8b-4c96-fafc-8da6739ec7ba"
      },
      "source": [
        "print('This DataFrame Has %d Rows and %d Columns'%(dataframe.shape))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This DataFrame Has 351 Rows and 35 Columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9mfI4xcvhM7"
      },
      "source": [
        "We print the first five rows of our dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewYsB4SBt2tX",
        "outputId": "79a29f3b-e8da-44ae-8e44-4a9bb3900619"
      },
      "source": [
        "print(dataframe.head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0   1        2        3        4        5        6        7        8        9        10  \\\n",
            "0   1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000  0.03760  0.85243   \n",
            "1   1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000 -0.04549  0.50874   \n",
            "2   1   0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965  0.01198  0.73082   \n",
            "3   1   0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000  0.00000  0.00000   \n",
            "4   1   0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152 -0.16399  0.52798   \n",
            "\n",
            "        11       12       13       14       15       16       17       18       19       20  \\\n",
            "0 -0.17755  0.59755 -0.44945  0.60536 -0.38223  0.84356 -0.38542  0.58212 -0.32192  0.56971   \n",
            "1 -0.67743  0.34432 -0.69707 -0.51685 -0.97515  0.05499 -0.62237  0.33109 -1.00000 -0.13151   \n",
            "2  0.05346  0.85443  0.00827  0.54591  0.00299  0.83775 -0.13644  0.75535 -0.08540  0.70887   \n",
            "3  0.00000  0.00000  0.00000 -1.00000  0.14516  0.54094 -0.39330 -1.00000 -0.54467 -0.69975   \n",
            "4 -0.20275  0.56409 -0.00712  0.34395 -0.27457  0.52940 -0.21780  0.45107 -0.17813  0.05982   \n",
            "\n",
            "        21       22       23       24       25       26       27       28       29       30  \\\n",
            "0 -0.29674  0.36946 -0.47357  0.56811 -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267   \n",
            "1 -0.45300 -0.18056 -0.35734 -0.20332 -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626   \n",
            "2 -0.27502  0.43385 -0.12062  0.57528 -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436   \n",
            "3  1.00000  0.00000  0.00000  1.00000  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682   \n",
            "4 -0.35575  0.02309 -0.52879  0.03286 -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707   \n",
            "\n",
            "        31       32       33 34  \n",
            "0 -0.54487  0.18641 -0.45300  g  \n",
            "1 -0.06288 -0.13738 -0.02447  b  \n",
            "2 -0.24180  0.56045 -0.38238  g  \n",
            "3  1.00000 -0.32382  1.00000  b  \n",
            "4 -0.59573 -0.04608 -0.65697  g  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwOKJrY1t5wt",
        "outputId": "2f64c40f-c8a9-4090-f7db-35a5f3d828ef"
      },
      "source": [
        "# We isolate the features matrix from the DataFrame.\n",
        "features_matrix = dataframe.iloc[:, 0:34]\n",
        "\n",
        "# We isolate the target vector from the DataFrame.\n",
        "target_vector = dataframe.iloc[:, -1]\n",
        "\n",
        "# We check the shape of the features matrix, and target vector.\n",
        "print('The Features Matrix Has %d Rows And %d Column(s)'%(features_matrix.shape))\n",
        "print('The Target Matrix Has %d Rows And %d Column(s)'%(np.array(target_vector).reshape(-1, 1).shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Features Matrix Has 351 Rows And 34 Column(s)\n",
            "The Target Matrix Has 351 Rows And 1 Column(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MptQbTYpvl4r"
      },
      "source": [
        "We use scikit-learn's `StandardScaler` in order to\n",
        "preprocess the features matrix data. This will\n",
        "ensure that all values being inputted are on the same\n",
        "scale for the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvrejAkat6AG"
      },
      "source": [
        "features_matrix_standardized = StandardScaler().fit_transform(features_matrix)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2H5f9CyvpZm"
      },
      "source": [
        "We create an instance of the Logistic Regression Algorithm\n",
        "We utilize the default values for the parameters and\n",
        "hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJX-w_hht9Ir"
      },
      "source": [
        "algorithm = LogisticRegression(penalty='l2', dual=False, tol=1e-4,\n",
        "C=1.0, fit_intercept=True,\n",
        "intercept_scaling=1, class_weight=None,\n",
        "random_state=None, solver='lbfgs',\n",
        "max_iter=100, multi_class='auto',\n",
        "verbose=0, warm_start=False, n_jobs=None,\n",
        "l1_ratio=None)\n",
        "\n",
        "# We utilize the 'fit' method in order to conduct the\n",
        "# training process on our features matrix and target vector.\n",
        "\n",
        "Logistic_Regression_Model = algorithm.fit(features_matrix_standardized, target_vector)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Jw0YXUvuIh"
      },
      "source": [
        "We create an observation with values, in order\n",
        "to test the predictive power of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsbCS7Bxt-ao"
      },
      "source": [
        "observation = [[1, 0, 0.99539, -0.05889, 0.8524299999999999, 0.02306,\n",
        "0.8339799999999999, -0.37708, 1.0, 0.0376,\n",
        "0.8524299999999999, -0.17755, 0.59755, -0.44945, 0.60536,\n",
        "-0.38223, 0.8435600000000001, -0.38542, 0.58212, -0.32192,\n",
        "0.56971, -0.29674, 0.36946, -0.47357, 0.56811, -0.51171,\n",
        "0.41078000000000003, -0.46168000000000003, 0.21266, -0.3409,\n",
        "0.42267, -0.54487, 0.18641, -0.453]]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpybiIlzvwjY"
      },
      "source": [
        "We store the predicted class value in a variable\n",
        "called '`predictions`'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfeYjubQuBUk"
      },
      "source": [
        "predictions = Logistic_Regression_Model.predict(observation)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tBLHNS-v0Ml"
      },
      "source": [
        "We print the model's predicted class for the observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6wq0YbWuClC",
        "outputId": "aa9d5db1-29b5-4a1b-dbbe-390844ef4b52"
      },
      "source": [
        "print('The Model Predicted The Observation To Belong To Class %s'%(predictions))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Model Predicted The Observation To Belong To Class ['g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BiRgSqJv2TW"
      },
      "source": [
        "We view the specific classes the model was trained to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tppWhQ1uEJN",
        "outputId": "04b1610c-6f9e-4feb-d01f-5437b495d9d1"
      },
      "source": [
        "print('The Algorithm Was Trained To Predict One Of The Two Classes: %s'%(algorithm.classes_))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Algorithm Was Trained To Predict One Of The Two Classes: ['b' 'g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kBDtr-auF1Q",
        "outputId": "2e05da92-ed45-4c7a-e2a2-91beb241894b"
      },
      "source": [
        "print(\"\"\"The Model Says The Probability Of The Observation We Passed Belonging To Class ['b'] Is %s\"\"\"%(algorithm.predict_proba(observation)[0][0]))\n",
        "print()\n",
        "print(\"\"\"The Model Says The Probability Of The Observation We Passed Belonging To Class ['g'] Is %s\"\"\"%(algorithm.predict_proba(observation)[0][1]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Model Says The Probability Of The Observation We Passed Belonging To Class ['b'] Is 0.0077739316001402825\n",
            "\n",
            "The Model Says The Probability Of The Observation We Passed Belonging To Class ['g'] Is 0.9922260683998597\n"
          ]
        }
      ]
    }
  ]
}